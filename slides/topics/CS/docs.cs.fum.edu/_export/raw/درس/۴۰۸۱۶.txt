
---- اطلاعات درس ----

شماره: ۴۰۸۱۶
عنوان: امنیت و حریم خصوصی در یادگیری ماشین
عنوان انگلیسی: Security and Privacy of Machine Learning
مقطع: کارشناسی ارشد
گرایش: رایانش امن
واحد: ۳
نوع: نظری
دسته: تخصصی
پیش‌نیاز: آمار و احتمال مهندسی
هم‌نیاز: --
متولی: گروه فناوری اطلاعات
طراح:‌ دکتر سیدامیرمهدی صادق‌زاده
آخرین تصویب: ۱۳۹۸/۰۱/۰۱

قالب: قالب درس
----------------------------


===== اهداف درس =====
هدف از این درس آشنایی دانشجویان با امنیت و حریم خصوصی در یادگیری ماشین و نحوه افزایش مقاومت الگوریتم‌های یادگیری ماشین در برابر مهاجمان است. با افزایش حجم داده ها و پیشرفت الگوریتم‌های یادگیری ماشین، استفاده از یادگیری ماشین در کاربردهای مختلفی مانند پردازش تصویر، پردازش صدا، پردازش متن، سامانه‌های تصمیم‌گیر، و سامانه‌های امنیتی رشد زیادی داشته است. افزایش استفاده از یادگیری ماشین در کاربردهای گوناگون باعث شده است تا اهمیت امنیت و حریم خصوصی در الگوریتم های یادگیری ماشین افزایش یابد. در این درس دانشجویان با مهمترین آسیب‌پذیری‌های امنیتی یادگیری ماشین مانند نمونه خصمانه و مسمومیت داده‌های آموزش در کاربردهای گوناگون مانند پردازش تصویر و صدا آشنا می‌شوند و راهکار‌های مقاوم‌سازی مدل یادگیری ماشین در برابر این حملات نیز معرفی می‌شوند. همچنین مفهوم حریم خصوصی در یادگیری ماشین معرفی می شود و ضعف‌ الگوریتم‌های یادگیری ماشین در نشت اطلاعات مهم و نحوه حفظ حریم خصوصی در این الگوریتم‌ها بررسی می‌شود. به دلیل عملکرد درخشان یادگیری ژرف تمرکز این درس بر روی این الگوریتم‌ها است.
===== ریز مواد =====
  * **مبانی یادگیری ماشین**
    * تعریف انواع الگوریتم‌های یادگیری ماشین (با نظارت، بدون نظارت، و نیمه‌نظارتی)
    * معرفی معیارهای ارزیابی الگوریتم‌های یادگیری ماشین
  * **یادگیری ژرف**
    * معرفی دسته‌بندهای خطی
    * مبانی شبکه‌های عصبی 
    * الگوریتم گرادیان کاهشی
    * تابع هزینه و الگوریتم انتشار به عقب
    * معرفی شبکه‌های عصبی پیچشی و انواع ان
  * **حملات نمونه خصمانه و روش‌های افزایش مقاومت مدل در برابر آن‌ها**
    * معرفی نمونه‌های خصمانه
    * قابلیت انتقال نمونه‌های خصمانه
    * نمونه‌های خصمانه در کاربردهای گوناگون
    * نمونه‌های خصمانه در دنیای واقعی
    * نمونه‌های خصمانه جعبه-سیاه
    * آموزش خصمانه
    * درستی‌یابی مدل
    * تشخیص نمونه‌های خصمانه
  * **حملات مسموم ‌سازی دادگان آموزش و روش‌‌های دفاعی در برابر آن‌ها**
    * حملات مسموم‌ سازی دادگان آموزش
    * افزایش مقاومت مدل در مقابل حملات مسموم‌ سازی دادگان آموزش
  * **حملات سرقت مدل و روش‌های دفاعی در برابر آن‌ها**
  * **حریم خصوصی در یادگیری ماشین**
    * معرفی مفهوم حریم خصوصی در یادگیری ماشین
    * حمله استنتاج عضویت
  * **یادگیری ماشین حافظ حریم خصوصی**
    * حریم‌خصوصی تفاضلی
    * حریم‌خصوصی تفاضلی در یادگیری ماشین

===== ارزیابی =====

  * مرور مقالات: ۶ نمره
  * تکالیف: ۶ نمره
  * پروژه: ۲ نمره
  * پایان‌ترم: ۶ نمره

===== مراجع =====

<div :en>
  -	C. M. Bishop and C. M., Pattern recognition and machine learning. Springer, 2006.
  -	I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. The MIT Press, 2016.
  -	C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus “Intriguing properties of neural networks,” in Proceedings of the International Conference on Representation Learning (ICLR), 2014.
  -	I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing Adversarial Examples,” in Proceedings of the International Conference on Representation Learning (ICLR), 2015.
  -	N. Carlini and D. Wagner, “Towards Evaluating the Robustness of Neural Networks,” in 2017 IEEE Symposium on Security and Privacy (SP), 2017, pp. 39–57.
  -	Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into Transferable Adversarial Examples and Black-box Attacks,” in Proceedings of the International Conference on Representation Learning (ICLR), 2017.
  -	S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal Adversarial Perturbations,” in EEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1765–1773.
  -	N. Carlini and D. Wagner, “Audio Adversarial Examples: Targeted Attacks on Speech-to-Text,” in 2018 IEEE Security and Privacy Workshops (SPW), 2018, pp. 1–7.
  -	M. Osadchy, J. Hernandez-Castro, S. Gibson, O. Dunkelman, and D. Perez-Cabo, “No Bot Expects the DeepCAPTCHA! Introducing Immutable Adversarial Examples, With Applications to CAPTCHA Generation,” IEEE Trans. Inf. Forensics Secur., vol. 12, no. 11, pp. 2640–2653, Nov. 2017.
  -	K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel, “Adversarial Examples for Malware Detection,” in uropean Symposium on Research in Computer Security, 2017, pp. 62–79.
  -	N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou, “Hidden voice commands,” in 25th USENIX Security Symposium, 2016, pp. 513–530.
  -	A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples in the physical world,” in Proceedings of the International Conference on Representation Learning (ICLR), 2017.
  -	D. Song, K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, F. Tramer, A. Prakash, and T. Kohno., “Physical Adversarial Examples for Object Detectors,” in 12th USENIX Workshop on Offensive Technologies (WOOT18), 2018.
  -	P. Chen, H. Zhang, Y. Sharma, J. Yi, and C. Hsieh. "Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models." In Proceedings of the 10th ACM workshop on artificial intelligence and security, 2017, pp. 15-26. 
  -	A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. "Black-box adversarial attacks with limited queries and information." In International Conference on Machine Learning, 2018, pp. 2137-2146.
  -	B. Wang and N. Z. Gong, “Stealing Hyperparameters in Machine Learning,” in 2018 IEEE Symposium on Security and Privacy (SP), 2018, pp. 36–52.
  -	F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing Machine Learning Models via Prediction APIs,” in 25th USENIX Security Symposium, 2016, pp. 601–618.
  -	A. Shafahi, W. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, “Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,” in Advances in Neural Information Processing Systems 31, 2018, pp. 6103–6113.
  -	B. Tran, J. Li, and A. Madry, “Spectral Signatures in Backdoor Attacks,” in Advances in Neural Information Processing Systems 31, 2018, pp. 8000–8010.
  -	A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards Deep Learning Models Resistant to Adversarial Attacks,” in Proceedings of the International Conference on Representation Learning (ICLR), 2018.
  -	F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, “Ensemble Adversarial Training: Attacks and Defenses,” in Proceedings of the International Conference on Representation Learning (ICLR), 2017.
  -	N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks,” in 2016 IEEE Symposium on Security and Privacy (SP), 2016, pp. 582–597.
  -	A. Sinha, H. Namkoong, and J. Duchi, “Certifying Some Distributional Robustness with Principled Adversarial Training,” in Proceedings of the International Conference on Representation Learning (ICLR), 2017.
  -	G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, “Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks,” in International Conference on Computer Aided Verification, 2017, pp. 97–117.
  -	D. Meng and H. Chen, “MagNet: a Two-Pronged Defense against Adversarial Examples,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security - CCS ’17, 2017, pp. 135–147.
  -	N. Baracaldo, B. Chen, H. Ludwig, and J. A. Safavi, “Mitigating Poisoning Attacks on Machine Learning Models,” in Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security  - AISec ’17, 2017, pp. 103–110.
  -	J. Steinhardt, P. Koh, and P. Liang, “Certified Defenses for Data Poisoning Attacks,” in Advances in Neural Information Processing Systems 30, 2017, pp. 3517–3529.
  -	R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership Inference Attacks Against Machine Learning Models,” in 2017 IEEE Symposium on Security and Privacy (SP), 2017, pp. 3–18.
  -	C. Song, T. Ristenpart, and V. Shmatikov, “Machine Learning Models that Remember Too Much,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security  - CCS ’17, 2017, pp. 587–601.
  -	C. Dwork and A. Roth, “The Algorithmic Foundations of Differential Privacy,” Found. Trends. Theor. Comput. Sci., vol. 9, no. 3–4, pp. 211–407, 2013.
  -	R. Shokri and V. Shmatikov, “Privacy-Preserving Deep Learning,” in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security - CCS ’15, 2015, pp. 1310–1321.
  -	M. Abadi et al., “Deep Learning with Differential Privacy,” in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security - CCS’16, 2016, pp. 308–318.
  -	P. Mohassel and Y. Zhang, “SecureML: A System for Scalable Privacy-Preserving Machine Learning,” in 2017 IEEE Symposium on Security and Privacy (SP), 2017, pp. 19–38.
</div>
